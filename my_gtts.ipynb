{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from gtts import gTTS\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = '/Users/prithvik/Documents/USC/Assignments/EE541/EE541 Project/India.gif' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from gtts import gTTS\n",
    "from torchvision import models\n",
    "import os\n",
    "\n",
    "# Load the pre-trained model and set device\n",
    "ResNet_dict = torch.load('tuned_resnet_model.pth',map_location=torch.device('cpu'))\n",
    "ResNet = models.resnet50()\n",
    "num_features = ResNet.fc.in_features\n",
    "ResNet.fc = nn.Linear(num_features, 29)\n",
    "ResNet.load_state_dict(ResNet_dict)\n",
    "\n",
    "# Define the transformation for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load and preprocess the GIF file\n",
    "gif_path = '/Users/prithvik/Documents/USC/Assignments/EE541/EE541 Project/India.gif'  # Replace with the path to your GIF file\n",
    "gif = Image.open(gif_path)\n",
    "frame_rate = gif.info['duration'] / 1000 \n",
    "frames = []\n",
    "for frame in range(0, gif.n_frames, 69):\n",
    "    gif.seek(frame)\n",
    "    image = gif.convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    frames.append(image)\n",
    "\n",
    "# Make predictions for each frame and transcribe into speech\n",
    "predicted_letters = []\n",
    "for image in frames:\n",
    "    with torch.no_grad():\n",
    "        output = ResNet(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    predicted_letter = chr(ord('A') + predicted.item())  # Convert label to letter\n",
    "    predicted_letters.append(predicted_letter)\n",
    "\n",
    "# Convert predicted letters into a sentence\n",
    "sentence = ' '.join(predicted_letters)\n",
    "\n",
    "# Generate speech from the sentence using gTTS\n",
    "tts = gTTS(text=sentence, lang='en')\n",
    "audio_path = 'output.mp3'  # Path to save the generated audio file\n",
    "tts.save(audio_path)\n",
    "\n",
    "# Play the generated audio\n",
    "os.system(f'afplay {audio_path}')  # Works on macOS, change command for other platforms if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted letter: I\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pre-trained model\n",
    "ResNet_dict = torch.load('tuned_resnet_model.pth',map_location=torch.device('cpu'))\n",
    "ResNet = models.resnet50()\n",
    "num_features = ResNet.fc.in_features\n",
    "ResNet.fc = nn.Linear(num_features, 29)\n",
    "ResNet.load_state_dict(ResNet_dict)\n",
    "\n",
    "# Define the transformation for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = '/Users/prithvik/Documents/USC/Assignments/EE541/Project/data/asl_alphabet_train/asl_alphabet_train/Y/Y6.jpg'  # Replace with the path to your image\n",
    "image = Image.open(image_path)\n",
    "image = transform(image).unsqueeze(0)\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    output = ResNet(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "# Convert the predicted label to the corresponding letter\n",
    "label_mapping = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "predicted_letter = label_mapping[predicted.item()]\n",
    "\n",
    "# Print the predicted letter\n",
    "print('Predicted letter:', predicted_letter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m ResNet \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mresnet50()\n\u001b[1;32m     12\u001b[0m num_features \u001b[39m=\u001b[39m ResNet\u001b[39m.\u001b[39mfc\u001b[39m.\u001b[39min_features\n\u001b[0;32m---> 13\u001b[0m ResNet\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(num_features, \u001b[39m29\u001b[39m)\n\u001b[1;32m     14\u001b[0m ResNet\u001b[39m.\u001b[39mload_state_dict(ResNet_dict)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Define the transformation for the input image\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "\n",
    "# Load the pre-trained model\n",
    "ResNet_dict = torch.load('tuned_resnet_model.pth',map_location=torch.device('cpu'))\n",
    "ResNet = models.resnet50()\n",
    "num_features = ResNet.fc.in_features\n",
    "ResNet.fc = nn.Linear(num_features, 29)\n",
    "ResNet.load_state_dict(ResNet_dict)\n",
    "\n",
    "# Define the transformation for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load label mapping\n",
    "label_mapping = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "# Start video capture from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for letter prediction\n",
    "current_letter = None\n",
    "prev_letter = None\n",
    "\n",
    "while True:\n",
    "    # Read frame from webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to PIL image\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Make a prediction\n",
    "    with torch.no_grad():\n",
    "        output = ResNet(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "    # Get the predicted letter\n",
    "    current_letter = label_mapping[predicted.item()]\n",
    "\n",
    "    # Play letter audio if it's a new letter\n",
    "    if current_letter != prev_letter:\n",
    "        prev_letter = current_letter\n",
    "\n",
    "        # Convert letter to speech\n",
    "        tts = gTTS(text=current_letter, lang='en')\n",
    "        tts.save('temp.mp3')\n",
    "\n",
    "        # Play the audio\n",
    "        playsound('temp.mp3')\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('ASL Transcription', frame)\n",
    "\n",
    "    # Check for 'q' key press to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee541",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
